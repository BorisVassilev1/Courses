{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 52912\n",
      "RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png A\n",
      "SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png A\n",
      "Um9tYW5hIEJvbGQucGZi.png A\n",
      "B 52912\n",
      "TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png B\n",
      "C 52912\n",
      "D 52912\n",
      "VHJhbnNpdCBCb2xkLnR0Zg==.png D\n",
      "E 52912\n",
      "F 52912\n",
      "G 52912\n",
      "H 52912\n",
      "I 52912\n",
      "J 52911\n",
      "529114\n",
      "A 1872\n",
      "B 1873\n",
      "C 1873\n",
      "D 1873\n",
      "E 1873\n",
      "F 1872\n",
      "G 1872\n",
      "H 1872\n",
      "I 1872\n",
      "J 1872\n",
      "18724\n",
      "(529114, 784) (529114, 10)\n",
      "(18724, 784) (18724, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = list(), list()\n",
    "x_test, y_test = list(), list()\n",
    "\n",
    "train_letters = os.listdir(\"./notMNIST_large\")\n",
    "test_letters = os.listdir(\"./notMNIST_small\")\n",
    "\n",
    "count = 0;\n",
    "for i, letter in enumerate(train_letters):\n",
    "    fonts = os.listdir(\"./notMNIST_large/\" + letter)\n",
    "    print(str(letter)+ \" \" + str(len(fonts)))\n",
    "    for j, font in enumerate(fonts):\n",
    "        img = None\n",
    "        try:\n",
    "            img = plt.imread(\"./notMNIST_large/\"+ letter + \"/\" + font)\n",
    "        except:\n",
    "            print(font + \" \" + letter)\n",
    "        if img is None :\n",
    "            count -= 1\n",
    "            continue\n",
    "        x_train.append(img.reshape([28*28]))\n",
    "        y_train.append(np.zeros(10))\n",
    "        y_train[count + j][i] = 1\n",
    "    count += len(fonts)\n",
    "print(count)\n",
    "\n",
    "count = 0;\n",
    "for i, letter in enumerate(test_letters):\n",
    "    fonts = os.listdir(\"./notMNIST_small/\" + letter)\n",
    "    print(str(letter)+ \" \" + str(len(fonts)))\n",
    "    for j, font in enumerate(fonts):\n",
    "        img = None\n",
    "        try:\n",
    "            img = plt.imread(\"./notMNIST_small/\"+ letter + \"/\" + font)\n",
    "        except:\n",
    "            print(font + \" \" + letter)\n",
    "            #plt.imshow(img)\n",
    "        #plt.imshow(img)\n",
    "        if img is None :\n",
    "            count -= 1\n",
    "            continue\n",
    "        x_test.append(img.reshape([28*28]))\n",
    "        y_test.append(np.zeros(10))\n",
    "        y_test[count + j][i] = 1\n",
    "    count += len(fonts)\n",
    "print(count)\n",
    "\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "x_test = np.asarray(x_test)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(a,b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = shuffle(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = int(np.random.rand(1) * 18724)\n",
    "plt.imshow(x_train[i].reshape([28,28]))\n",
    "print(y_train[i])\n",
    "time.sleep(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    layers = list()\n",
    "    \n",
    "    w = list()\n",
    "    b = list()\n",
    "    \n",
    "    a = list()\n",
    "    z = list()\n",
    "    \n",
    "    da = list()\n",
    "    db = list()\n",
    "    dw = list()\n",
    "    \n",
    "    activators = list()\n",
    "    \n",
    "    depth = 0\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0,x)\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def drelu(self, x):\n",
    "        return x > 0\n",
    "    \n",
    "    def dsigmoid(self, x):\n",
    "        sigx = self.sigmoid(x)\n",
    "        return np.multiply(sigx, (1 - sigx))\n",
    "    \n",
    "    def __init__(self, input_size, layers_count, layers_size, activators, l_rate = 0.01):\n",
    "        self.layers = list()\n",
    "        self.w = list()\n",
    "        self.b = list()\n",
    "        self.a = list()\n",
    "        self.z = list()\n",
    "        \n",
    "        self.activators = activators\n",
    "        self.l_rate = l_rate\n",
    "        \n",
    "        self.layers.append(input_size)\n",
    "        self.a.append(0)\n",
    "        self.z.append(0)\n",
    "        for x in range(0,layers_count):\n",
    "            self.layers.append(layers_size[x])\n",
    "            self.a.append(0)\n",
    "            self.z.append(0)\n",
    "        \n",
    "        self.depth = len(self.layers)\n",
    "        \n",
    "        for x in range(1, self.depth):\n",
    "            #matarr = np.ndarray(shape=(self.layers[x-1], self.layers[x]))\n",
    "            matarr = np.random.normal(0,0.5, (self.layers[x-1], self.layers[x]))\n",
    "            mat = np.matrix(matarr)\n",
    "            self.w.append(mat)\n",
    "            #print(mat)\n",
    "            #barr = np.ndarray(self.layers[x])\n",
    "            barr = np.random.normal(0,0.5, self.layers[x])\n",
    "            #print(barr.shape)\n",
    "            #barr = barr.reshape((barr.shape[0], 1))\n",
    "            self.b.append(barr)\n",
    "        \n",
    "        \n",
    "    def predict(self,input):\n",
    "        self.a[0] = input\n",
    "        #print(self.a[0])\n",
    "        \n",
    "        for x in range(1,self.depth):\n",
    "            self.z[x] = np.matmul(self.a[x - 1], self.w[x - 1]) + self.b[x - 1]\n",
    "            if self.activators[x - 1] == \"sig\":\n",
    "                val = self.sigmoid(self.z[x])\n",
    "            elif self.activators[x - 1] == \"relu\":\n",
    "                val = self.relu(self.z[x])\n",
    "            self.a[x] = val\n",
    "            #print(\"wtf \" + str(x) + str(self.a[x]))\n",
    "        return self.a[self.depth - 1]\n",
    "    \n",
    "    def test(self, input, ans):\n",
    "        prediction = self.predict(input)\n",
    "        #c1 = np.array(list(np.array_equal(np.round(prediction[x]), ans[x]) for x in range(ans.shape[0])))\n",
    "        c1 = np.array(list(np.argmax(prediction[x]) == np.argmax(ans[x]) for x in range(ans.shape[0])))\n",
    "        #print(prediction)\n",
    "        \n",
    "        c = prediction - ans\n",
    "        c = np.square(c)\n",
    "        c = c.sum(axis = 1).mean()\n",
    "        return {\"mean_loss\": c, \"accuracy\": c1.sum() / c1.shape[0]}\n",
    "    \n",
    "    def train(self, input, ans):\n",
    "        prediction = self.predict(input)\n",
    "        #print(input.shape[0])\n",
    "        c = (prediction - ans)\n",
    "        c = np.square(c)\n",
    "        #print(c)\n",
    "        \n",
    "        c1 = np.array(list(np.argmax(prediction[x]) == np.argmax(ans[x]) for x in range(ans.shape[0])))\n",
    "        \n",
    "        prediction = np.swapaxes(prediction, 0,1)\n",
    "        ans = np.swapaxes(ans, 0,1)\n",
    "        \n",
    "        self.da = list() # dC(0)/da(Lj)\n",
    "        self.db = list() # dC(0)/db(Lj)\n",
    "        self.dw = list() # dC(0)/dw(Ljk)\n",
    "        \n",
    "        for i, x in enumerate(self.layers):\n",
    "            self.da.append(0)\n",
    "            self.db.append(0)\n",
    "            self.dw.append(0)\n",
    "        #print(len(self.db))\n",
    "        self.da[self.depth - 1] = 2 * (ans - prediction)\n",
    "        #print(self.da[self.depth - 1])\n",
    "        \n",
    "        #print(\"a\")\n",
    "        for x in range(self.depth - 2, 0, -1):\n",
    "            #print(x)\n",
    "            #print(self.w[x].shape, self.da[x+1].shape)\n",
    "            #a = np.asarray(np.matmul(self.w[x], da[x + 1]))\n",
    "            a = np.matmul(self.w[x], self.da[x + 1])\n",
    "            #print(a.shape)\n",
    "            #print(type(a))\n",
    "            #print(type(self.z[x]))\n",
    "            #print(a.shape, self.z[x].shape)\n",
    "            if self.activators[x - 1] == \"sig\":\n",
    "                #a = np.multiply(a, np.asarray(self.dsigmoid(self.z[x])))\n",
    "                a = np.multiply(a, np.swapaxes(self.dsigmoid(self.z[x]), 0, 1))\n",
    "            elif self.activators[x - 1] == \"relu\":\n",
    "                #a = np.multiply(a, np.asarray(self.drelu(self.z[x])))\n",
    "                a = np.multiply(a, np.swapaxes(self.drelu(self.z[x]), 0, 1))\n",
    "            #print(a)\n",
    "            #print(a.shape)\n",
    "            self.da[x] = a\n",
    "        #print(\"b\")\n",
    "        for x in range(1, self.depth):\n",
    "            #print(x)\n",
    "            if self.activators[x - 1] == \"sig\":\n",
    "                #a = np.multiply(a, np.asarray(self.dsigmoid(self.z[x])))\n",
    "                b = np.multiply(self.da[x], np.swapaxes(self.dsigmoid(self.z[x]), 0, 1))\n",
    "            elif self.activators[x - 1] == \"relu\":\n",
    "                #a = np.multiply(a, np.asarray(self.drelu(self.z[x])))\n",
    "                b = np.multiply(self.da[x], np.swapaxes(self.drelu(self.z[x]), 0, 1))\n",
    "            #print(b)\n",
    "            \n",
    "            self.db[x] = b\n",
    "            self.db[x] = np.mean(self.db[x], axis = 1)\n",
    "            #print(self.db[x].shape)\n",
    "            \n",
    "        #print(\"w\")\n",
    "        for x in range(1, self.depth):\n",
    "            \n",
    "            \n",
    "            _a = np.swapaxes(self.a[x - 1], 0, 1)\n",
    "            \n",
    "            w = np.zeros((_a.shape[0], self.da[x].shape[0], _a.shape[1]))\n",
    "            \n",
    "            for k in range(_a.shape[0]):\n",
    "                for j in range(self.da[x].shape[0]):\n",
    "                    w[k][j] = np.multiply(_a[k], self.da[x][j])\n",
    "            \n",
    "            self.dw[x] = w\n",
    "            self.dw[x] = np.mean(self.dw[x], axis = 2)\n",
    "            #print(self.dw[x].shape)\n",
    "        \n",
    "        for x in range(1, self.depth):\n",
    "            self.w[x - 1] += self.dw[x] * self.l_rate\n",
    "            \n",
    "            self.b[x - 1] = np.reshape(self.b[x - 1], (self.b[x - 1].shape[0], 1))\n",
    "            self.b[x - 1] += self.db[x] * self.l_rate\n",
    "            self.b[x - 1] = np.squeeze(self.b[x - 1])\n",
    "        \n",
    "        return {\"mean_loss\": c.sum(axis = 1).mean(), \"accuracy\": c1.sum() / c1.shape[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(784,2,(16,10), (\"relu\", \"sig\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = nn.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_loss': 3.9177419137503233, 'accuracy': 0.08638970051822481}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.train(x_train[:100], y_train[:100])\n",
    "nn.test(x_train, y_train)\n",
    "#nn.dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stating Training on 529114 samples, aka 529 batches. 5 epochs total.\n",
      "Epoch: 1 Finished! Train loss: 0.27492802182167714 Train accuracy:  0.8100598358765786 Test loss: 0.18688345182854657 Test accuracy: 0.8773766289254433\n",
      "Epoch: 2 Finished! Train loss: 0.27489151783191584 Train accuracy:  0.8097253143934955 Test loss: 0.18581495720512564 Test accuracy: 0.8764687032685323\n",
      "Epoch: 3 Finished! Train loss: 0.27373173907191684 Train accuracy:  0.8096761756445681 Test loss: 0.18532903261057496 Test accuracy: 0.8765221106601153\n",
      "Epoch: 4 Finished! Train loss: 0.2714261074532969 Train accuracy:  0.8121633523210499 Test loss: 0.18365791788797534 Test accuracy: 0.8782845545823542\n",
      "Epoch: 5 Finished! Train loss: 0.2716192421427193 Train accuracy:  0.812032945641204 Test loss: 0.18418403137119482 Test accuracy: 0.8783913693655202\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "epochs = 5\n",
    "nn.l_rate = 0.01\n",
    "\n",
    "batches = x_train.shape[0] // batch_size\n",
    "print(\"Stating Training on\", x_train.shape[0], \"samples, aka\", batches, \"batches.\", epochs, \"epochs total.\")\n",
    "#batch_length = int(np.floor(x_train.shape[0] / (batches)))\n",
    "for y in range(epochs):\n",
    "    for x in range(batches + 1):\n",
    "        train_result = None\n",
    "        if x == batches:\n",
    "            if x_train.shape[0] != batch_size * batches:\n",
    "                train_result = nn.train(x_train[batch_size * batches:], y_train[batch_size * batches:])\n",
    "            break\n",
    "        train_result = nn.train(x_train[batch_size * x : batch_size * (x + 1) - 1], y_train[batch_size * x : batch_size * (x + 1) - 1])\n",
    "        #print(\"a\")\n",
    "        print(\"Epoch:\", y + 1, \"Batch:\", x + 1, \"Batch loss:\", train_result[\"mean_loss\"], \"Batch accuracy:\", train_result[\"accuracy\"], end=\"\\r\")\n",
    "    test_result = nn.test(x_test, y_test)\n",
    "    train_result = nn.test(x_train, y_train)\n",
    "    print(\"Epoch:\", y + 1, \"Finished!\" ,\"Train loss:\", train_result[\"mean_loss\"], \"Train accuracy: \", train_result[\"accuracy\"], \"Test loss:\", test_result[\"mean_loss\"], \"Test accuracy:\", test_result[\"accuracy\"])\n",
    "    x_train, y_train = shuffle(x_train,y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
